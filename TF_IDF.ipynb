{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SOW3pKkGXZI"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "import operator\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "Stopwords = set(stopwords.words('english'))\n",
        "wordlemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e-9DqSkbGi6Y"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"You can learn a lot about yourself through travelling. You can observe how you feel being far from your\n",
        "country. You will find out how you feel about your homeland. You will realize how you really feel about\n",
        "foreign people. You will find out how much you know/do not know about the world. You will be able to\n",
        "observe how you react in completely new situations. You will test your language, orientational and\n",
        "social skills. You will not be the same person after returning home. During travelling you will meet\n",
        "people that are very different from you. If you travel enough, you will learn to accept and appreciate\n",
        "these differences. Traveling makes you more open and accepting.\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Equd_j4SGk43"
      },
      "outputs": [],
      "source": [
        "def clean(text):\n",
        "    regex = r'[^a-zA-Z0-9\\s]'\n",
        "    text = re.sub(regex,'',text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kChqBKoHGueN"
      },
      "outputs": [],
      "source": [
        "def freq(words):\n",
        "    words = [word.lower() for word in words]\n",
        "    dict_freq = {}\n",
        "    words_unique = []\n",
        "    for word in words:\n",
        "       if word not in words_unique:\n",
        "          words_unique.append(word)\n",
        "    for word in words_unique:\n",
        "          dict_freq[word] = words.count(word)\n",
        "    return dict_freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y2xmEXZBHHl"
      },
      "outputs": [],
      "source": [
        "def importance(sentence,dict_freq,sentences):\n",
        "     sentence_score = 0\n",
        "     sentence = remove_special_characters(str(sentence))\n",
        "     sentence = re.sub(r'\\d+', '', sentence)\n",
        "     tagged_sentence = []\n",
        "     no_of_sentences = len(sentences)\n",
        "     tagged_sentence = tagging(sentence)\n",
        "     for word in tagged_sentence:\n",
        "         if word.lower() not in Stopwords and word not in Stopwords    and len(word)>1:\n",
        "             word = word.lower() \n",
        "             word = wordlemmatizer.lemmatize(word)\n",
        "             sentence_score = sentence_score + word_tfidf(dict_freq,word,sentences,sentence)\n",
        "     return sentence_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WScxarwVBL7m"
      },
      "outputs": [],
      "source": [
        "def tagging(text):\n",
        "    pos_tag = nltk.pos_tag(text.split())\n",
        "    pos_tagged_noun_verb = []\n",
        "    for word,tag in pos_tag:\n",
        "        if tag == \"NN\" or tag == \"NNP\" or tag == \"NNS\" or tag == \"VB\" or tag == \"VBD\" or tag == \"VBG\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VBZ\":\n",
        "            pos_tagged_noun_verb.append(word)\n",
        "    return pos_tagged_noun_verb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6PwVqpmBYGj"
      },
      "outputs": [],
      "source": [
        "def word_tfidf(dict_freq,word,sentences,sentence):\n",
        "    word_tfidf = []\n",
        "    tf = tf_score(word,sentence)\n",
        "    idf = idf_score(len(sentences),word,sentences)\n",
        "    tf_idf = tf_idf_score(tf,idf)\n",
        "    return tf_idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6A6mbCmBafs"
      },
      "outputs": [],
      "source": [
        "def tf_score(word,sentence):\n",
        "    freq_sum = 0\n",
        "    word_frequency_in_sentence = 0\n",
        "    len_sentence = len(sentence)\n",
        "    for word_in_sentence in sentence.split():\n",
        "        if word == word_in_sentence:\n",
        "            word_frequency_in_sentence = word_frequency_in_sentence + 1\n",
        "    tf =  word_frequency_in_sentence/ len_sentence\n",
        "    return tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLsQm24aBq1i"
      },
      "outputs": [],
      "source": [
        "def idf_score(no_of_sentences,word,sentences):\n",
        "    no_of_sentence_containing_word = 0\n",
        "    for sentence in sentences:\n",
        "        sentence = remove_special_characters(str(sentence))\n",
        "        sentence = re.sub(r'\\d+', '', sentence)\n",
        "        sentence = sentence.split()\n",
        "        sentence = [word for word in sentence if word.lower() not in Stopwords and len(word)>1]\n",
        "        sentence = [word.lower() for word in sentence]\n",
        "        sentence = [wordlemmatizer.lemmatize(word) for word in sentence]\n",
        "        if word in sentence:\n",
        "            no_of_sentence_containing_word = no_of_sentence_containing_word + 1\n",
        "    idf = math.log10(no_of_sentences/no_of_sentence_containing_word)\n",
        "    return idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FgLZDxA9HEXB"
      },
      "outputs": [],
      "source": [
        "def lemmatizer(words):\n",
        "    lemmatized_words = []\n",
        "    for word in words:\n",
        "       lemmatized_words.append(wordlemmatizer.lemmatize(word))\n",
        "    return lemmatized_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqIJ87jxItUD"
      },
      "outputs": [],
      "source": [
        "def tf_idf_score(tf,idf):\n",
        "    return tf*idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEn5IgyqMnrT",
        "outputId": "b3e6fc6c-ef41-4bc1-8e5e-e90e8226a0ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Summary:\n",
            "You will find out how much you know/do not know about the world. You will not be the same person after returning home. If you travel enough, you will learn to accept and appreciate\n",
            "these differences.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\EXTRA\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\EXTRA\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "199"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "tokenized_sentence = sent_tokenize(text)\n",
        "text = clean(str(text))\n",
        "text = re.sub(r'\\d+', '', text)\n",
        "tokenized_words_with_stopwords = word_tokenize(text)\n",
        "tokenized_words = [word for word in tokenized_words_with_stopwords if word not in Stopwords]\n",
        "tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
        "tokenized_words = [word.lower() for word in tokenized_words]\n",
        "tokenized_words = lemmatizer(tokenized_words)\n",
        "word_freq = freq(tokenized_words)\n",
        "input_user = 30\n",
        "no_of_sentences = int((input_user * len(tokenized_sentence))/100)\n",
        "c = 1\n",
        "sentence_with_importance = {}\n",
        "for sent in tokenized_sentence:\n",
        "    sentenceimp = importance(sent,word_freq,tokenized_sentence)\n",
        "    sentence_with_importance[c] = sentenceimp\n",
        "    c = c+1\n",
        "sentence_with_importance = sorted(sentence_with_importance.items(), key=operator.itemgetter(1),reverse=True)\n",
        "cnt = 0\n",
        "summary = []\n",
        "sentence_no = []\n",
        "for word_prob in sentence_with_importance:\n",
        "    if cnt < no_of_sentences:\n",
        "        sentence_no.append(word_prob[0])\n",
        "        cnt = cnt+1\n",
        "    else:\n",
        "        break\n",
        "sentence_no.sort()\n",
        "cnt = 1\n",
        "for sentence in tokenized_sentence:\n",
        "    if cnt in sentence_no:\n",
        "       summary.append(sentence)\n",
        "    cnt = cnt+1\n",
        "summary = \" \".join(summary)\n",
        "print(\"\\n\")\n",
        "print(\"Summary:\")\n",
        "print(summary)\n",
        "outF = open('summary.txt',\"w\")\n",
        "outF.write(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pbFbRy9k40jn"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CyGK3NWg40jo"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "TF-IDF.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}